<h1>Notes about approach (for report):</h1>

<h2>Block generation</h2>

<h3>Sequential approach</h3>
<p>
<ul>
<li>Approached project by first designing working naive solution (before thinking about sequential optimisation at all)</li>

<li>-As blocks are generated, they are added to a growing pool of blocks (dyanmically reallocated to fit the new blocks), which is later used in collision detection</li>

<li>Blocks are represented using a struct, which simply stores the signature of the block and the column in which it was found</li>
</ul>
</p>

<h3>Some thoughts on parallelisation that came up: </h3>
<p>
<ul>
<li>Naive solution finds blocks with nested for loop (outer loop, 3 inner loops O(n^4) for each column). The solution loops over all the columns, and executes the four inner loops to find all blocks. Thus each column is independent of one another</li>

<li>Given that columns are independent, I initially thought of generating a task for each column, however, this proved to be an inefficient methodology once I discovered that most blocks lay in the last (500th) column. So splitting work by column would not have been efficient</li>
</ul>
</p>

<h3> First attempt at parallelisation: Parallelising within columns</h3>
<p>
<ul>
<li>Next attempt at parallelisation was to use omp pragmas to parallelise the outermost for loop of each column. i.e. within each column, split the block generating work up among each thread by giving each thread a chunk of rows to analyse. Since the work at each row is independent, this didn't hamper correctness, however, there were a few issues</li>

<li>This approach was taken to counter the aforementioned issue of most blocks residing in the last column. If parallelisation distributes work within each column (rather than distributing by columns), then the last column can still be distributed among the thread pool</li>

<li>An immediate issue with this approach is that the parallel region begins and ends within each column's individual iteration. This means that <i>#pragma omp parallel</i>, and the barrier at the end of the parallel region, are encountered once per column (so 500 times for this data set). Since spawning new threads incurs overhead, this is quite an inefficient way to parallelise</li>

<li>The second issue is that each thread must maintain its own private database of blocks to which it updates (so as to avoid incoherency in the data by writing to shared database). This means that these partial databases must be merged with the complete database at the end of each thread's work on a column. This requires a critical region (which is very damaging to performance) to ensure the data in the collective database is coherent. Since this critical region is encountered once per thread per column, it is certainly going to be a bottleneck in performance. Overall, it would be encountered 4 times per column (assuming 4 threads) and thus 2000 times during overall block generation process</li>

<li>Despite these inefficiencies, the parallelisation was still effective in improving performance. For the first 499 columns, it achieved a speed up of ~2.2 (~37 seconds for sequential, ~17 seconds for parallel). However, the parallel code's performance took a drastic hit at the last column, and was rapidly overtaken by the sequential code.</li>

<li>Believing the issue to be cache issue, I attempted to resolve it by performing the parallelisation of the for loops manually. Instead of using <i>#pragma omp for</i>, I instead had each thread acquire its thread ID number, and the number of threads, at the beginning of the parallel region. I used <i>omp_get_thread_num()</i> and <i>omp_get_num_threads()</i> to achieve this. I then had each thread proceed through the for loop according to index numbers calculated using the aforementioned <i>thread number</i> and <i>number of threads</i>. Each thread started with an index equal to its <i>thread number</i>, and instead of incrementing by 1 on each iteration, it incremented by <i>number of threads</i>. So with 2 threads, this would be akin to giving one thread all even numbers, and the other thread all odd numbers. I believed this would minimize the number of cache misses. However, no significant performance improvement was observed.</li>

</ul>
</p>

<h3> Second attempt at parallelisation: Isolating last column and writing special case for it</h3>
<p>
<ul>
<li>Attempted to split last column using tasks. Created task for each 1000-row block of the last column, to have different segments of the column worked on simulateneously. This drastically improved performance compared to previous parallel attempt, however, parallel code was still comparatively slow. i.e. the parallel code ran much faster for the last column than the previous parallel attempt, but still ran slightly slower than sequential code. Overall, with speedup in first 499 columns and slowdown in last column, parallel code ran almost identically to sequential code</li>

<li>Made the observation that it was a load balancing issue. Most blocks within last column were within the first 1000 rows, and beyond that, there were far more blocks between rows 1000-2000 then there were between 2000-4000. This being the case, I realized that splitting the rows (in the last column) by chunks of 1000 would cause load balancing issues, and this could be observed in the output. The threads that took the tasks for rows 2000-3000 and 3000-4000 would finish substantially faster than the other 2 threads on rows 0-1000 and 1000-2000. These threads would finish the two available tasks for rows 2000-3000 and 3000-4000 far earlier than either of the other 2 threads. At this point, there would be no more tasks left (since only 4 were generated to begin with), and so only 2 threads would be executing simulatenously, wasting the opportunity for 2 threads to be doing work. Additionally, the thread in charge of rows 1000-2000 would then finish much earlier than the thread in charge of rows 0-1000, leaving only 1 thread executing.</li>

<li>Following this observation, I attempted to improve load balancing by reducing the chunk size for each task. I then also attempted to create more tasks for the first 2000 rows, and less for the last 2000. While this did improve load balancing (all threads would be executing at any one time for almost the entire execution period), it actually significantly reduced performance. This is when I made the observation that each thread's individual performance would improve substantially when no other threads were executing. i.e. now that all threads were executing simultaenously, each thread was individually finding blocks much, much slower, and so overall performance fell drastically. Conversely, when the load balancing was uneven, the thread performing the most work (i.e. rows 0-1000) would be executing alone for most of the time, so that's why it would get through them much quicker, and why performance before improving load-balancing was better than it was after.</li>

<li>This last observation was a strong hint to me that the slowdown was being caused by cache mismanagement issues in the code. Because threads would run faster when no other threads were executing, this suggested that the threads were consistently interfering with each other during work on the last column. The most likely reason for this would be cache mismanagement; the multi-threading attept was probably increasing the number of cashe-misses, and the ratio of cache-hits:cache-misses would improve when only one thread was active. My next task was thus to explore the memory access patterns of the code within the tasks for the last column</li>
</ul>
</p>

<h3> Issues observed with code </h3>
<p>
<ul>
<li>Observed that since block database was storing pointers to blocks, rather than blocks, there would be lots of cache misses in collision detection (which loops through the block database). The reason is that while the pointers are all stored contiguously in main memory, the blocks themselves are not; they are in dynamically allocated spots in main memory, so each reference to a subsequent block in the database will likely cause a cache miss</li>

<li>Observed that the loop over columns would likely cause lots of cache misses, since it would mean the inner loop is over rows, and this would likely mean that each new memory reference is to a piece of the matrix that doesn't exist in cache</li>
</ul>
</p>

<h3> Attempts made to rectify cache unfriendliness in code </h3>
<p>
<ul>
<li> Following from the above observations, I transposed the matrix (i.e. created a new matrix whose rows equaled the old matrix's columns) and used that new transposed matrix for the block generation. This way, the outer loop in block generation (while still over the columns of the data set, in a logical sense), would be over the rows of the transposed matrix, increasing cache friendliness. This drastically improved performance, decreasing execution time (for parallel code) for columns 0-498 from about 17 seconds down to roughly 7 seconds</li>
</ul>
</p>

<h3> First timing of collision detection </h3>
<p>
<ul>
<li> Timed entire program (as it is so far, which generates all blocks and all collisions), and it took <b>1 minute 33 seconds</b> to run. The block generation takes roughly <b>7 seconds</b (note these tests are being carried out without the 499th block), so this means collision detection is taking roughly <b>1 minute 25 seconds</b> to run. Collision detection is still sequential</li>

<li> Will attempt to speed up collision detection by making block database a single array of blocks, (i.e a pointer to a block) rather than an array of pointers to blocks (i.e a pointer to a pointer to a block). This should increase the cache friendliness of the collision detection code (which loops through all blocks in the blockDB), as there are likely to be far less cache misses</li>

<li> Made block database contiguous, and did the same to collision database. So they’re now types <i>Block *</i> and <i>Collision *</i> instead of <i>Block **</i> and <i>Collision **</i>. Correctness doesn’t seem to have been impacted. Also, collisions no longer store an array of blocks (per collision). Instead, each collision only stores the columns of the colliding blocks. i.e. if 3 blocks across 3 different columns have the same signature, then the 3 column numbers will be stored in the collision struct, rather than the 3 blocks themselves.</li>

<li> After making block array contiguous, execution sped up very slightly (5-10 seconds max). Will now work on parallelising collision detection</li>
</ul>
</p>

<h3> First attempt at parallelising collision detection </h3>
<p>
<ul>
<li> </li>
</ul>
